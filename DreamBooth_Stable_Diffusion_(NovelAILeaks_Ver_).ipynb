{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TimoZhou1024/AI-Painting/blob/main/DreamBooth_Stable_Diffusion_(NovelAILeaks_Ver_).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XU7NuMAA2drw"
      },
      "outputs": [],
      "source": [
        "#@title Check type of GPU and VRAM available.\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLWXPZqjsZVV",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install Requirements\n",
        "\n",
        "# !curl -LO https://github.com/huggingface/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
        "# use memory-optim version\n",
        "!curl -Lo train_dreambooth.py https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/dreambooth.py\n",
        "!pip install -U pip\n",
        "\n",
        "!pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
        "!pip install -qq -U --pre triton\n",
        "!pip install -qq accelerate==0.12.0 transformers ftfy bitsandbytes gradio\n",
        "!pip install -qq omegaconf einops pytorch_lightning "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Install xformers from precompiled wheel.\n",
        "from subprocess import getoutput\n",
        "from IPython.display import HTML\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "s = getoutput('nvidia-smi')\n",
        "if 'T4' in s:\n",
        "  gpu = 'T4'\n",
        "elif 'P100' in s:\n",
        "  gpu = 'P100'\n",
        "elif 'V100' in s:\n",
        "  gpu = 'V100'\n",
        "elif 'A100' in s:\n",
        "  gpu = 'A100'\n",
        "\n",
        "while True:\n",
        "    try: \n",
        "        gpu=='T4'or gpu=='P100'or gpu=='V100'or gpu=='A100'\n",
        "        break\n",
        "    except:\n",
        "        pass\n",
        "    print('[1;31mit seems that your GPU is not supported at the moment')\n",
        "    time.sleep(5)\n",
        "\n",
        "if (gpu=='T4'):\n",
        "  %pip install -qq https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "  \n",
        "elif (gpu=='P100'):\n",
        "  %pip install -qq https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "\n",
        "elif (gpu=='V100'):\n",
        "  %pip install -qq https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "\n",
        "elif (gpu=='A100'):\n",
        "  %pip install -qq https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl  "
      ],
      "metadata": {
        "cellView": "form",
        "id": "yB-qGga6AyPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download `animefull` model and traindata (nahida)\n",
        "\n",
        "#@markdown Path of model\n",
        "URL = \"https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/animefull-pruned.tar\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Path of the initial model folder.\n",
        "SRC_PATH = \"/content/animefull-pruned\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Path of the convert output (as train model input).\n",
        "MODEL_NAME = \"/content/animefull\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Download example training data (nahida)?\n",
        "download_example_traindata = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Use external `animevae.pt`?\n",
        "include_animevae = True #@param {type:\"boolean\"}\n",
        "\n",
        "!mkdir -p $SRC_PATH\n",
        "%cd $SRC_PATH\n",
        "!apt install -y -qq aria2 > /dev/null\n",
        "!aria2c --summary-interval=5 -x 6 --allow-overwrite=true -o data.tar $URL\n",
        "!echo \"Decompressing..\"\n",
        "!tar xf data.tar \n",
        "!echo \"Done\"\n",
        "\n",
        "%cd /content\n",
        "!curl -Lo convert.py https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/convert_v2.py\n",
        "!curl -Lo /content/animevae.pt https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/animevae.pt\n",
        "\n",
        "if include_animevae:\n",
        "  !python convert.py --checkpoint_path $SRC_PATH/model.ckpt --original_config_file $SRC_PATH/config.yaml --vae_path animevae.pt --dump_path $MODEL_NAME --scheduler_type ddim\n",
        "else:\n",
        "  !python convert.py --checkpoint_path $SRC_PATH/model.ckpt --original_config_file $SRC_PATH/config.yaml --dump_path $MODEL_NAME --scheduler_type ddim\n",
        "\n",
        "if download_example_traindata:\n",
        "  !mkdir -p nahida\n",
        "  !curl -L https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/train-nahida.tar | tar x -C nahida > /dev/null\n",
        "  !rm -fr nahida/._train*\n"
      ],
      "metadata": {
        "id": "VYQ9Mt1pizKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rxg0y5MBudmd"
      },
      "outputs": [],
      "source": [
        "#@title Settings\n",
        "\n",
        "#@markdown Path and prompt for images of the concept for training.\n",
        "INSTANCE_PROMPT = \"cute nahida\" #@param {type:\"string\"}\n",
        "INSTANCE_DIR = \"/content/nahida\" #@param {type:\"string\"}\n",
        "!mkdir -p $INSTANCE_DIR\n",
        "\n",
        "#@markdown A general name and prompt for class like dog for dog images.\n",
        "CLASS_PROMPT = \"full body illustration of a anime cute face young girl in dramatic light, masterpiece and best quality.\" #@param {type:\"string\"}\n",
        "CLASS_NEGATIVE_PROMPT = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\" #@param {type:\"string\"}\n",
        "CLASS_DIR = \"/content/nahida_autogen\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Prompt for saving samples.\n",
        "SAVE_SAMPLE_PROMPT = \"nahida sitting on the chair, masterpiece and best quality.\" #@param {type:\"string\"}\n",
        "SAVE_SAMPLE_NEGATIVE_PROMPT = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown If model weights should be saved directly in google drive (takes around 4-5 GB).\n",
        "save_to_gdrive = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Enter the directory name to save model at.\n",
        "OUTPUT_DIR = \"nahida-dreambooth-output\" #@param {type:\"string\"}\n",
        "\n",
        "if save_to_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "if save_to_gdrive:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Metrics\n",
        "\n",
        "#@markdown Use external services to monitor training.\n",
        "use_tensorboard = False #@param {type:\"boolean\"}\n",
        "use_wandb = False #@param {type:\"boolean\"}\n",
        "save_weights_to_wandb = False #@param {type:\"boolean\"}\n",
        "wandb_apikey = \"\" #@param {type:\"string\"}\n",
        "\n",
        "if use_wandb:\n",
        "  %pip install -qq git+https://github.com/wandb/wandb\n",
        "  if wandb_apikey == \"\":\n",
        "    raise ValueError('Invalid wandb.ai APIKey')\n",
        "  !wandb login $wandb_apikey\n",
        "\n",
        "if use_tensorboard:\n",
        "  # Tensorboard support\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir $OUTPUT_DIR/logs"
      ],
      "metadata": {
        "id": "gxKaSYUcfDMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn5ILIyDJIcX"
      },
      "source": [
        "### Start Training\n",
        "\n",
        "Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
        "\n",
        "\n",
        "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
        "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
        "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
        "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
        "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
        "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
        "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
        "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
        "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ioxxvHoicPs"
      },
      "source": [
        "Add `--gradient_checkpointing` flag for around 9.92 GB VRAM usage.\n",
        "\n",
        "remove `--use_8bit_adam` flag for full precision. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjcSXTp-u-Eg"
      },
      "outputs": [],
      "source": [
        "#@markdown Use the keywords included in each instance images as unique instance prompt.\n",
        "read_prompt_from_filename = False #@param {type:\"boolean\"}\n",
        "read_prompt_from_txt = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Training parameters.\n",
        "resolution = 448 #@param {type:\"slider\", min:64, max:2048, step:32}\n",
        "train_batch_size = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "seed = 1337 #@param {type:\"number\"}\n",
        "num_class_images = 150 #@param {type:\"number\"}\n",
        "max_train_steps = 2000 #@param {type:\"number\"}\n",
        "save_interval = 500 #@param {type:\"number\"}\n",
        "log_interval = 10 #@param {type:\"number\"}\n",
        "lr_scheduler = \"cosine_with_restarts\"  #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"] {allow-input: false}\n",
        "learning_rate = 5e-6 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "wandb_arg = \"\"\n",
        "save_sample_arg = \"\"\n",
        "extra_prompt_arg = \"\"\n",
        "\n",
        "if read_prompt_from_filename:\n",
        "  extra_prompt_arg = \"--read_prompt_filename\"\n",
        "\n",
        "if read_prompt_from_txt:\n",
        "  extra_prompt_arg = \"--read_prompt_txt\"\n",
        "\n",
        "if use_wandb:\n",
        "  wandb_arg = \"--wandb\"\n",
        "\n",
        "if save_weights_to_wandb:\n",
        "  wandb_arg = \"--wandb --wandb_artifact\"\n",
        "\n",
        "%cd /content\n",
        "!mkdir -p $OUTPUT_DIR\n",
        "!accelerate launch train_dreambooth.py \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --instance_data_dir=$INSTANCE_DIR \\\n",
        "  --class_data_dir=$CLASS_DIR \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --instance_prompt=\"{INSTANCE_PROMPT}\" \\\n",
        "  --class_prompt=\"{CLASS_PROMPT}\" \\\n",
        "  --class_negative_prompt=\"{CLASS_NEGATIVE_PROMPT}\" \\\n",
        "  --save_sample_prompt={SAVE_SAMPLE_PROMPT} \\\n",
        "  --save_sample_negative_prompt={SAVE_SAMPLE_NEGATIVE_PROMPT} \\\n",
        "  --seed=$seed \\\n",
        "  --resolution=$resolution \\\n",
        "  --train_batch_size=$train_batch_size \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=$learning_rate \\\n",
        "  --lr_scheduler=$lr_scheduler \\\n",
        "  --lr_warmup_steps=100 \\\n",
        "  --num_class_images=$num_class_images \\\n",
        "  --sample_batch_size=4 \\\n",
        "  --max_train_steps=$max_train_steps \\\n",
        "  --save_interval=$save_interval \\\n",
        "  --log_interval=$log_interval \\\n",
        "  --gradient_checkpointing \\\n",
        "  --use_8bit_adam $wandb_arg $save_sample_arg $extra_prompt_arg\n",
        "\n",
        "# disabled: --not_cache_latents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89Az5NUxOWdy"
      },
      "outputs": [],
      "source": [
        "#@title Convert weights to ckpt to use in web UIs like AUTOMATIC1111.\n",
        "\n",
        "!curl -Lo back_convert.py https://github.com/huggingface/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "!curl -Lo back_convert_alt.py https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/back_convert.py\n",
        "\n",
        "#@markdown Run conversion.\n",
        "ckpt_path = OUTPUT_DIR + \"/model.ckpt\"\n",
        "src_ckpt_path = SRC_PATH + \"/model.ckpt\"\n",
        "use_checkpoint = 'checkpoint_last' #@param {type:\"string\"}\n",
        "use_alt_method = False  #@param {type:\"boolean\"}\n",
        "half_precision = False  #@param {type:\"boolean\"}\n",
        "\n",
        "if use_alt_method:\n",
        "  !python back_convert_alt.py --model_path '{OUTPUT_DIR}/checkpoint_last' --src_path $src_ckpt_path  --checkpoint_path $ckpt_path \n",
        "else:\n",
        "  if half_precision:\n",
        "    !python back_convert.py --model_path '{OUTPUT_DIR}/checkpoint_last' --checkpoint_path $ckpt_path --half\n",
        "  else:\n",
        "    !python back_convert.py --model_path '{OUTPUT_DIR}/checkpoint_last' --checkpoint_path $ckpt_path \n",
        "\n",
        "\n",
        "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW15FjffdTID"
      },
      "outputs": [],
      "source": [
        "#@title Inference\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from IPython.display import display\n",
        "\n",
        "#@markdown Run Gradio UI for generating images.\n",
        "use_checkpoint = 'checkpoint_last' #@param {type:\"string\"}\n",
        "model_path = os.path.join(OUTPUT_DIR, use_checkpoint)\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16).to(\"cuda\")\n",
        "g_cuda = None\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def inference(prompt, negative_prompt, num_samples, height=512, width=512, num_inference_steps=50, guidance_scale=7.5):\n",
        "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
        "        return pipe(\n",
        "                prompt, height=int(height), width=int(width),\n",
        "                negative_prompt=negative_prompt,\n",
        "                num_images_per_prompt=int(num_samples),\n",
        "                num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n",
        "                generator=g_cuda\n",
        "            ).images\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of sks guy, digital painting\")\n",
        "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n",
        "            run = gr.Button(value=\"Generate\")\n",
        "            with gr.Row():\n",
        "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
        "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n",
        "            with gr.Row():\n",
        "                height = gr.Number(label=\"Height\", value=512)\n",
        "                width = gr.Number(label=\"Width\", value=512)\n",
        "            num_inference_steps = gr.Slider(label=\"Steps\", value=50)\n",
        "        with gr.Column():\n",
        "            gallery = gr.Gallery()\n",
        "\n",
        "    run.click(inference, inputs=[prompt, negative_prompt, num_samples, height, width, num_inference_steps, guidance_scale], outputs=gallery)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}